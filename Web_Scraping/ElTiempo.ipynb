{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Notebook Fuente El tiempo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerías"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importan las librerías de interés para el desarrollo del Web Scraping de la fuente El Tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium as sel\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import base as bs\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "import os as os\n",
    "import csv\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingresar la empresa a extraer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empresa con la cual se va a extraer los artículos. Se ingresa la empresa a la cual se quiere ejecutar el Web Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, se ingresa también la cantidad de páginas de la fuente El Tiempo a la cual se quiere extraer.\n",
    "\n",
    "Es decir, por ejemplo si se ingresan N páginas, el Web Scraping extraerá todos los artículos que se encuentran en las N páginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingresa la empresa a la cual se quiere realizar el Web Scraping. Adicionalmente se utiliza en formato de mínusculas\n",
    "empresa = input(\"Digite la empresa a extraer: \").lower().strip()\n",
    "\n",
    "# Si ingresa una empresa que posee dos palabras o más. Reemplaza el espacio al equivalente URL que necesita la fuente El Tiempo\n",
    "if \" \" in empresa:\n",
    "    empresa = empresa.replace(\" \", \"%20\")\n",
    "\n",
    "# Ingresa la cantidad de páginas a realizar el Web Scraping del El Tiempo\n",
    "paginas = input(\"la cantidad de paginas \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium y Web Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activa el navegador de Microsoft Edge como el predeterminado.\n",
    "\n",
    "Adicionalmente, realiza la recolección de los datos de los artículos desplegados en la búsqueda de El Tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Abre el Navegador de Edge\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m driver \u001b[39m=\u001b[39m sel\u001b[39m.\u001b[39mwebdriver\u001b[39m.\u001b[39mEdge()\n\u001b[0;32m      3\u001b[0m driver\u001b[39m.\u001b[39mManage()\u001b[39m.\u001b[39mCookies\u001b[39m.\u001b[39mDeleteAllCookies()\n\u001b[0;32m      4\u001b[0m titulares\u001b[39m=\u001b[39m[]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sel' is not defined"
     ]
    }
   ],
   "source": [
    "# Abre el Navegador de Edge\n",
    "try:\n",
    "    driver = sel.webdriver.Edge()\n",
    "except:\n",
    "    cwd = os.getcwd()\n",
    "    path = os.path.join(cwd, 'msedgedriver.exe')\n",
    "    path.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "    driver = sel.webdriver.Edge(executable_path=path.replace(\"\\\\\\\\\", \"\\\\\"))\n",
    "titulares=[]\n",
    "\n",
    "# Hacer el WS del primer url\n",
    "bs.obtener_articulos_eltiempo(driver=driver, url=f'https://www.eltiempo.com/buscar?q={empresa}',titulares = titulares,empresa=empresa)\n",
    "\n",
    "# Realiza el WS de las demás páginas. El número de páginas ingresado anteriormente\n",
    "for i in range(2,int(paginas) + 1):\n",
    "        bs.obtener_articulos_eltiempo(driver=driver, url=f'https://www.eltiempo.com/buscar/{i}?q={empresa}',titulares = titulares,empresa=empresa)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrae info de cada URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa los cookies del navegador \n",
    "driver.delete_all_cookies()\n",
    "\n",
    "# Busca cada articulo y las almacena en la lista de titulares\n",
    "for tit in titulares:\n",
    "    \n",
    "    driver.get(tit['URL'])\n",
    "    driver.implicitly_wait(10) # Metodología de wait: espera a que el Navegador tenga cargado toda la página exitosamente\n",
    "\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
    "    contenido = ''\n",
    "    try :\n",
    "        html = WebDriverWait(driver,10,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH,'.//div[contains(@class,\"modulos public-side\")]')))\n",
    "        parrafos = html.find_elements(By.XPATH,'.//p') \n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            html = driver.find_element(By.XPATH,'.//div[contains(@class,\"modulos public-side\")]')\n",
    "            parrafos = html.find_elements(By.XPATH,'.//p')\n",
    "        except:\n",
    "            contenido = 'SIN PARRAFOS' # Si no hay contenido en dicho artículo\n",
    "        else:\n",
    "            for i in parrafos:\n",
    "                contenido += i.text\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            for i in parrafos:\n",
    "                contenido += i.text\n",
    "        except:\n",
    "            print('Error en el último for == el más dentro de \"for tit in titulares:\" externo')\n",
    "    \n",
    "    # agregar contenido al dict de titulares\n",
    "    tit['Contenido'] = contenido\n",
    "\n",
    "\n",
    "\n",
    "    # Para agregar el Autor del artículo\n",
    "    autor_eltiempo = ''\n",
    "    try:\n",
    "        autor_eltiempo = driver.find_element(By.XPATH,\"//div[(@class='author_data')]/div/a[@class='who']/span[@class='who']\").text\n",
    "    except:\n",
    "        autor_eltiempo = 'SIN AUTOR'\n",
    "\n",
    "    if(autor_eltiempo ==''):\n",
    "        try:\n",
    "            autor_eltiempo = driver.find_element(By.XPATH,\"//div[(@class='author_data')]/div/a[@class='who']/span[@class='who-modulo who']\").text\n",
    "        except:\n",
    "            autor_eltiempo = 'SIN AUTOR'\n",
    "\n",
    "    tit['Autor'] = autor_eltiempo\n",
    "\n",
    "\n",
    "\n",
    "    #Sacar la imagen del artículo\n",
    "\n",
    "    imagen=''\n",
    "\n",
    "    try:\n",
    "        imagen_temp = driver.find_element(By.XPATH, \"//div[@class='recurso_apertura']\")\n",
    "    except:\n",
    "        imagen = 'SIN IMAGEN'\n",
    "    else:\n",
    "        imagen = imagen_temp.find_element(By.XPATH, './/img').get_attribute('src')\n",
    "\n",
    "    tit['Imagen']=imagen\n",
    "\n",
    "    # URL de Noticias relacionadas\n",
    "    relNewsUrls = []\n",
    "\n",
    "    driver.delete_all_cookies() # clear all cookies in scope of session\n",
    "    \n",
    "    #agregar lista de URLs de noticias relacionadas\n",
    "    # tit['RelNewsUrls'] = bs.obtener_articulos_relacionados_eltiempo(driver)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para agregar la info en un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cerrar navegador\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(titulares)\n",
    "df['Empresa'] = df['Empresa'].str.replace('%20',' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac32b2f398e7b55f94ec93b78e14522600a25a69fa0bae156585a5bf7ae653de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
