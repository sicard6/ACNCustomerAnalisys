{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "import selenium as sel\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import os as os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def existedb(url: str):\n",
    "    \"\"\"Funcion que verifica si una url existe en la base de datos\n",
    "    Args:\n",
    "        url (str): url del articulo a verificar\n",
    "\n",
    "    Returns:\n",
    "        Bool: False si la url existe, True si no existe\n",
    "    \"\"\"\n",
    "    \n",
    "    db = pd.read_csv(\"../data/raw/noticias.csv\",encoding='utf8')\n",
    "    return True if (db[\"URL\"].eq(url)).any() else False\n",
    "    # with open('../data/raw/articulos.csv', encoding=\"utf8\") as f:\n",
    "    #     csvreader = csv.reader(f, delimiter=\",\")\n",
    "    #     for row in csvreader:\n",
    "    #         if url in row[5]:\n",
    "    #              return False\n",
    "    #     return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_autor(driver: sel.webdriver.Edge):\n",
    "    \"\"\"Funcion que obtiene el autor del articulo\n",
    "\n",
    "    Args:\n",
    "        driver (sel.webdriver.Edge): driver de selenium\n",
    "\n",
    "    Returns:\n",
    "        str: Nombre del autor del articulo\n",
    "    \"\"\"\n",
    "    autor = ''\n",
    "    try :\n",
    "        secAutor = driver.find_element(By.XPATH,'.//div[contains(@class,\"author-article\")]')\n",
    "    except:\n",
    "        autor = 'SIN AUTOR'\n",
    "    else:\n",
    "        try:\n",
    "            autor = secAutor.find_element(By.XPATH,'.//button').text\n",
    "        except:\n",
    "            autor = secAutor.find_element(By.XPATH,'.//span').text\n",
    "            \n",
    "    return autor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_articulos_relacionados(driver: sel.webdriver.Edge):\n",
    "    \"\"\"Obtener los articulos relacionados a un articulo\n",
    "\n",
    "    Args:\n",
    "        driver (sel.webdriver.Edge): referencia al driver de selenium\n",
    "\n",
    "    Returns:\n",
    "        List: lista de articulos relacionados\n",
    "    \"\"\"\n",
    "    relNewsUrls = []\n",
    "    try :\n",
    "        related = driver.find_elements(By.XPATH,'.//div[contains(@class,\"relatedNews\")]')\n",
    "        for i in related:\n",
    "            relNewsUrls.append(i.find_element(By.XPATH,'.//a').get_attribute('href'))\n",
    "    except:\n",
    "        relNewsUrls = []\n",
    "    return relNewsUrls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_contenido(driver: sel.webdriver.Edge):\n",
    "    \"\"\"Funcion que itera sobre todos los parrafos del articulo y los extrae.\n",
    "\n",
    "    Args:\n",
    "        driver (sel.webdriver.Edge): driver de selenium\n",
    "\n",
    "    Returns:\n",
    "        str: devuelve el contenido del articulo\n",
    "    \"\"\"\n",
    "    contenido = ''\n",
    "    try :\n",
    "        html = driver.find_element(By.XPATH,'.//div[contains(@class,\"html-content\")]')\n",
    "        parrafos = html.find_elements(By.XPATH,'.//p')\n",
    "    except:\n",
    "        contenido = 'SIN PARRAFOS'\n",
    "    else:\n",
    "        for i in parrafos:\n",
    "            contenido += i.text\n",
    "       \n",
    "    return contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empresa con la cual vamos a extraer los articulos\n",
    "# TODO Tener una lista que itere por todos lo clientes\n",
    "empresa = input(\"Digite la empresa a extraer: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cerar driver... MODIFICAR DEPENDIENDO DEL NAVEGADOR\n",
    "driver = sel.webdriver.Edge()\n",
    "driver.get(f'https://www.larepublica.co/{empresa}')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrae la lista de todos los articulos de la pagina\n",
    "articulos = driver.find_elements(By.XPATH,'.//div[contains(@class,\"row news\")]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sacar 3 principales titulares\n",
    "titulares = []\n",
    "princip = driver.find_elements(By.XPATH,'.//div[contains(@class,\"first-news\")]')\n",
    "urlPrinc = princip.find_elements(By.XPATH,'.//a')[1].get_attribute('href')\n",
    "temaPrinc = princip.find_elements(By.XPATH,'.//a')[1].text\n",
    "fechaPrinc = princip.find_element(By.XPATH,'.//span').text\n",
    "tituloPrinc = princip.find_elements(By.XPATH,'.//a')[2].text\n",
    "imagenPrinc = princip.find_element(By.XPATH,'.//img').get_attribute('src')\n",
    "titulares.append({'Fecha Extraccion':datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    'Titulo':tituloPrinc,\n",
    "                    'Fecha Publicacion':fechaPrinc,\n",
    "                    'Tema':temaPrinc,\n",
    "                    'URL':url,\n",
    "                    'Imagen':imagen,\n",
    "                    'Empresa':empresa})\n",
    "\n",
    "#el resto, autor, resumen, contenido y relacionados se sacan entrando a la url\n",
    "#despues de guardar articulos normales. Mismo para las otras dos noticias principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itera por cada articulo y extrae la informacion (EN CASO DE QUE NO EXISTA ARCHIVO DE ALMACENAMIENTO ANTERIOR)\n",
    "for art in articulos:\n",
    "    url = art.find_elements(By.XPATH,'.//a')[1].get_attribute('href')\n",
    "    fechaP = art.find_element(By.XPATH,'.//span[@class = \"date-news\"]').text\n",
    "    tema = art.find_elements(By.XPATH,'.//a')[1].text\n",
    "    resumen = art.find_element(By.XPATH,'.//p').text\n",
    "    titulo = art.find_element(By.XPATH,'.//h2').text\n",
    "    imagen = art.find_elements(By.XPATH,'.//img')[0].get_attribute('src')\n",
    "    titulares.append({'Fecha Extraccion':datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    'Titulo':titulo,\n",
    "                    'Fecha Publicacion':fechaP,\n",
    "                    'Tema':tema,\n",
    "                    'Resumen':resumen,\n",
    "                    'URL':url,\n",
    "                    'Imagen':imagen,\n",
    "                    'Empresa':empresa})\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itera por cada articulo y extrae la informacion (CASO DE QUE YA EXISTA ARCHIVO DONDE SE ALMACENA)\n",
    "for art in articulos:\n",
    "    url = art.find_elements(By.XPATH,'.//a')[1].get_attribute('href')\n",
    "    if not(existedb(url)):\n",
    "        fechaP = art.find_element(By.XPATH,'.//span[@class = \"date-news\"]').text\n",
    "        tema = art.find_elements(By.XPATH,'.//a')[1].text\n",
    "        resumen = art.find_element(By.XPATH,'.//p').text\n",
    "        titulo = art.find_element(By.XPATH,'.//h2').text\n",
    "        imagen = art.find_elements(By.XPATH,'.//img')[0].get_attribute('src')\n",
    "        titulares.append({'Fecha Extraccion':datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        'Titulo':titulo,\n",
    "                        'Fecha Publicacion':fechaP,\n",
    "                        'Tema':tema,\n",
    "                        'Resumen':resumen,\n",
    "                        'URL':url,\n",
    "                        'Imagen':imagen,\n",
    "                        'Empresa':empresa})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# busca los autores de cada articulo y las almacena en la lista de titulares\n",
    "for tit in titulares:\n",
    "    \n",
    "    driver.get(tit['URL'])\n",
    "\n",
    "    # agregar autor al dict de titulares\n",
    "    tit['Autor'] = obtener_autor(driver)\n",
    "    \n",
    "    # agregar contenido al dict de titulares\n",
    "    tit['Contenido'] = obtener_contenido(driver)\n",
    "    \n",
    "    #agregar lista de URLs de noticias relacionadas\n",
    "    tit['RelNewsUrls'] = obtener_articulos_relacionados(driver)     \n",
    "\n",
    "    # se podria agregar un if resumen vacio, llamar a resumen. (para las 3 noticias principales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = ['Fecha Extraccion','Titulo', 'Fecha Publicacion','Tema','Resumen','URL','Imagen','Empresa','Autor','Contenido','RelNewsUrls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para agregar titulares a un archivo.\n",
    "with open('../data/raw/noticias.csv', 'a', newline='') as csv_file:\n",
    "    dict_object = csv.DictWriter(csv_file, fieldnames=columnas) \n",
    "  \n",
    "    dict_object.writerows(titulares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para crear un nuevo archivo con los titulares\n",
    "pd.json_normalize(titulares).to_csv(f'../data/raw/noticias.csv',index=False, encoding='utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
