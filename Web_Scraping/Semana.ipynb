{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "import selenium as sel\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import os as os\n",
    "import csv\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def existedb(url: str):\n",
    "    \"\"\"Funcion que verifica si una url existe en la base de datos\n",
    "    Args:\n",
    "        url (str): url del articulo a verificar\n",
    "\n",
    "    Returns:\n",
    "        Bool: False si la url existe, True si no existe\n",
    "    \"\"\"\n",
    "    \n",
    "    db = pd.read_csv(\"../data/raw/semana.csv\",encoding='utf8')\n",
    "    return True if (db[\"URL\"].eq(url)).any() else False\n",
    "    # with open('../data/raw/articulos.csv', encoding=\"utf8\") as f:\n",
    "    #     csvreader = csv.reader(f, delimiter=\",\")\n",
    "    #     for row in csvreader:\n",
    "    #         if url in row[5]:\n",
    "    #              return False\n",
    "    #     return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_tags(driver: sel.webdriver.Edge):\n",
    "    \"\"\"Funcion que obtiene los tags del articulo\n",
    "\n",
    "    Args:\n",
    "        driver (sel.webdriver.Edge): driver de selenium\n",
    "\n",
    "    Returns:\n",
    "        str[]: Lista de strings/tags\n",
    "    \"\"\"\n",
    "    tags = []\n",
    "    try :\n",
    "        secTags = driver.find_element(By.XPATH,'.//div[contains(@class,\"tags-list\")]')\n",
    "    except:\n",
    "        tags = 'SIN TAGS'\n",
    "    else:\n",
    "        units = secTags.find_elements(By.XPATH,'.//span')\n",
    "        for i in units:\n",
    "            tags.append(i.text)\n",
    "                  \n",
    "    return tags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_contenido(driver: sel.webdriver.Edge):\n",
    "    \"\"\"Funcion que itera sobre todos los parrafos del articulo y los extrae.\n",
    "\n",
    "    Args:\n",
    "        driver (sel.webdriver.Edge): driver de selenium\n",
    "\n",
    "    Returns:\n",
    "        str: devuelve el contenido del articulo\n",
    "    \"\"\"\n",
    "    ignored_exceptions=(NoSuchElementException,StaleElementReferenceException)\n",
    "    contenido = ''\n",
    "    try :\n",
    "        #html = driver.find_element(By.XPATH,'.//article[contains(@class,\"paywall\")]')\n",
    "        html = WebDriverWait(driver,10,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH,'.//article[contains(@class,\"paywall\")]')))\n",
    "        parrafos = html.find_elements(By.XPATH,'.//p')\n",
    "    except:\n",
    "        try:\n",
    "            html = WebDriverWait(driver,10,ignored_exceptions=ignored_exceptions).until(EC.presence_of_element_located((By.XPATH,'.//p[contains(@id,\"textId\")]')))\n",
    "            #html = driver.find_element(By.XPATH,'.//p[contains(@id,\"textId\")]')\n",
    "            parrafos = html.find_elements(By.XPATH,'.//p')\n",
    "        except:\n",
    "            contenido = 'SIN PARRAFOS'\n",
    "        else:\n",
    "            for i in parrafos:\n",
    "                contenido += i.text\n",
    "    else:\n",
    "        for i in parrafos:\n",
    "            contenido += i.text\n",
    "       \n",
    "    return contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_tema(driver: sel.webdriver.Edge):\n",
    "    \"\"\"Funcion que itera sobre todos los parrafos del articulo y los extrae.\n",
    "\n",
    "    Args:\n",
    "        driver (sel.webdriver.Edge): driver de selenium\n",
    "\n",
    "    Returns:\n",
    "        str: devuelve el contenido del articulo\n",
    "    \"\"\"\n",
    "    tema = ''\n",
    "    try :\n",
    "        tema = driver.find_element(By.XPATH,'.//h3').text\n",
    "    except:\n",
    "        try:\n",
    "            secc = driver.find_element(By.XPATH,'.//div[contains(@class,\"styles__Header-sc-1w6splk-3 jtSkhd hidden-md hidden-sm\")]')\n",
    "            tema = secc.find_element(By.XPATH,'.//h1').text\n",
    "        except:\n",
    "            tema = 'SIN TEMA' \n",
    "    return tema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empresa con la cual vamos a extraer los articulos\n",
    "# TODO Tener una lista que itere por todos lo clientes\n",
    "empresa = input(\"Digite la empresa a extraer: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cerar driver... MODIFICAR DEPENDIENDO DEL NAVEGADOR\n",
    "driver = sel.webdriver.Edge()\n",
    "driver.get(f'https://www.semana.com/buscador/?query={empresa}')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrae la lista de todos los articulos de la pagina\n",
    "articulos = driver.find_elements(By.XPATH,'.//div[contains(@class,\"queryly_item_row\")]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itera por cada articulo y extrae la informacion (EN CASO DE QUE NO EXISTA ARCHIVO DE ALMACENAMIENTO ANTERIOR)\n",
    "titulares = []\n",
    "for art in articulos:\n",
    "    url = art.find_element(By.XPATH,'.//a').get_attribute('href')\n",
    "    fechaP = art.find_element(By.XPATH,'.//div[contains(@style,\"margin-bottom:0px;color:#555;font-size:12px;\")]').text\n",
    "    resumen = art.find_element(By.XPATH,'.//div[contains(@class,\"queryly_item_description\")]').text\n",
    "    titulo = art.find_element(By.XPATH,'.//div[contains(@class,\"queryly_item_title\")]').text\n",
    "    txtImage = art.find_element(By.XPATH,'.//div[contains(@class,\"queryly_advanced_item_imagecontainer\")]').get_attribute('style')\n",
    "    imagen = 'https://www.semana.com'\n",
    "    imagen = imagen + txtImage.split(\"\\\"\")[1]\n",
    "    titulares.append({'Fecha Extraccion':datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    'Titulo':titulo,\n",
    "                    'Fecha Publicacion':fechaP,\n",
    "                    'Resumen':resumen,\n",
    "                    'URL':url,\n",
    "                    'Imagen':imagen,\n",
    "                    'Empresa':empresa})\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# busca los autores de cada articulo y las almacena en la lista de titulares\n",
    "for tit in titulares:\n",
    "    \n",
    "    driver.get(tit['URL'])\n",
    "\n",
    "    # agregar contenido al dict de titulares\n",
    "    tit['Contenido'] = obtener_contenido(driver)\n",
    "\n",
    "    # agregar tags al dict de titulares\n",
    "    tit['Tags'] = obtener_tags(driver)\n",
    "    \n",
    "    # agregar contenido al dict de titulares\n",
    "    tit['Tema'] = obtener_tema(driver)\n",
    "      \n",
    "\n",
    "    # se podria agregar un if resumen vacio, llamar a resumen. (para las 3 noticias principales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = ['Fecha Extraccion','Titulo', 'Fecha Publicacion','Tema','Resumen','URL','Imagen','Empresa','Contenido','Tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para agregar titulares a un archivo.\n",
    "with open('../data/raw/noticias.csv', 'a', newline='') as csv_file:\n",
    "    dict_object = csv.DictWriter(csv_file, fieldnames=columnas) \n",
    "  \n",
    "    dict_object.writerows(titulares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para crear un nuevo archivo con los titulares\n",
    "pd.json_normalize(titulares).to_csv(f'../data/raw/semana.csv',index=False, encoding='utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
