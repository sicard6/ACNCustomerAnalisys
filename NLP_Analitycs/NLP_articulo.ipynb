{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, pandas as pd, numpy as np, gensim, matplotlib.pyplot as plt, warnings, spacy\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \"\"\"Permite leer la información del WS, ajustar encoding convenientemente. \n",
    "       # TODO Encoding de raw data\n",
    "    Returns:\n",
    "        df (DataFrame()): Genera un solo DF con el compilado de todas las fuentes \".csv\"\n",
    "    \"\"\"\n",
    "    fuentes = list(pd.Series(os.listdir('../data/raw/'))[pd.Series(os.listdir('../data/raw/')).str.contains('.csv')])\n",
    "    df = pd.DataFrame(columns=['Fecha Extraccion', 'Titulo', 'Fecha Publicacion',\n",
    "                               'Resumen', 'URL','Imagen', 'Empresa', 'Contenido',\n",
    "                               'Tags', 'Tema', 'Autor','RelNewsUrls', 'Fuente'])\n",
    "    for fuente in fuentes:\n",
    "        if (fuente == \"larepublica.csv\"):\n",
    "            df_ = pd.read_csv(\"../data/raw/\"+fuente, sep = ',', encoding = 'latin_1')\n",
    "            df_['Fuente'] = fuente\n",
    "        else:\n",
    "            df_ = pd.read_csv(\"../data/raw/\"+fuente, sep = ',', encoding = None)\n",
    "            df_['Fuente'] = fuente\n",
    "        df = pd.concat([df, df_], axis=0)\n",
    "    # df = df[df['Fuente'] == 'semana.csv'] # TODO quitar linea al arreglar encoding de la republica\n",
    "    df = df.drop(df[df['Contenido'] == \"SIN PARRAFOS\"].index).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = spacy.load('es_core_news_lg')\n",
    "stop_words = es.Defaults.stop_words\n",
    "# stop_words.extend([]) # Agregar palabras de ser necesario\n",
    "a,b = 'áéíóúü','aeiouu'\n",
    "trans = str.maketrans(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc_text(df_, tipo, cliente_):\n",
    "    '''\n",
    "    Estandariza y elimina bugs de cada texto\n",
    "    Args:\n",
    "        df_ (Object): DataFrame resultado del WS\n",
    "        tipoc (Str): Tipo de segmento de informacion a analizar (\"Titulo\", \"Resumen\", \"Contenido\")\n",
    "        cliente_ (Str): Empresa clientes foco para analisis\n",
    "    output:\n",
    "        lemma_words (List): Lista con palabras lematizadas\n",
    "        lda_words: \n",
    "    '''\n",
    "    # Filtrado de informacion para analisis\n",
    "    texto = df_[\n",
    "        (df_.Empresa==cliente_)\n",
    "    ][tipo]\n",
    "    \n",
    "    texts = texto.str.lower()\n",
    "    texts = texts.str.replace(cliente_.lower(), '', regex=True)\n",
    "    texts = texts.str.replace('[0-9]', '', regex=True)\n",
    "    texts = texts.str.replace('\\W', ' ', regex=True).str.strip()\n",
    "    texts = texts.str.replace(' {2,}', ' ', regex=True)\n",
    "    texts = texts.apply(lambda x: x.translate(trans))\n",
    "    texts = texts.apply(\n",
    "        lambda x: ' '.join(pd.Series(re.split(' ', x))[~pd.Series(re.split(' ', x)).isin(stop_words)].tolist())\n",
    "    )\n",
    "    lda_words = texts.str.split('\\W')\n",
    "    return lda_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useLDA(texts, n):\n",
    "    \"\"\" Ejecuta LDA por texto analizado\n",
    "    Args:\n",
    "        texts (Object): Texto luego de pre-procesamiento\n",
    "        n (int): Numero de topics a encontrar\n",
    "    \"\"\"\n",
    "    dictionary = Dictionary(texts)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "    lda_model = LdaMulticore(bow_corpus, num_topics=n, id2word=dictionary, passes=2, workers=2)\n",
    "    string = ''\n",
    "    return lda_model.show_topics(num_topics=1, num_words=5, log=False, formatted=False)  # show_topics()[0][1]\n",
    "    # for idx, topic in lda_model.print_topics(-1):\n",
    "    #     string += '*'*50+'\\n'\n",
    "    #     string += f'Topic: {idx} \\nWords: {topic}'+'\\n'\n",
    "    # print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data()\n",
    "data_aux = []\n",
    "for i in range(data.shape[0]):\n",
    "    aux = []\n",
    "    for item in list(useLDA(texts = pre_proc_text(df_ = data, tipo = 'Contenido', cliente_ = \"Ecopetrol\").loc[[i,]], n = 3)[0][1]):\n",
    "        aux.append(item[0])\n",
    "    data_aux.append(aux)\n",
    "data['Topics'] = data_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/curated/curated_data.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
